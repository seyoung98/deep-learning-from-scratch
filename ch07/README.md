## 합성곱 신경망(Convolutional Neural Network)

- `합성곱 계층`과 `풀링 계층`
  - CNN 계층은 Conv-ReLU-(Pooling)
  - 출력에 가까운 층은 지금까지의 Affine-ReLU 구성
  - 마지막 출력 계층은 Affine-Softmax로 구성
- 각 계층 사이에 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 완전연결(Fully-connected) 신경망과 다름
- 패딩(Padding), 스트라이드(Stride)와 같은 CNN 고유의 용어 등장

### 완전연결(Fully-connected) 계층의 문제점

- 데이터의 형상이 무시됨
  - ex) 데이터가 이미지인 경우 이미지는 가로, 세로, 채널로 구성된 3차원, 하지만 `fully-connected layer`에 입력 시 1차원으로 flatten 해주어야함
  - 하지만 이미지인 경우 이 형상에는 소중한 정보가 담겨있음
    - ex) 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접하게 관련됨
    - 3차원 속에서 의미를 갖는 본질적인 패턴이 숨어있음
  - `Fully-connected layer`는 이 성질을 무시하고 `flatten`함

---

## 합성곱 계층(Conv)

- 데이터의 형상을 유지함
  - ex) 이미지도 3차원 데이터로 입력받으며, 다음 계층에도 3차원으로 전달함
- 따라서 CNN은 이미지처럼 형상을 가진 데이터를 제대로 이해가능함
- `특징 맵(feature map)`: 합성곱 계층의 입출력 데이터
  - `입력 특징 맵(input feature map)`: 입력 데이터
  - `출력 특징 맵(output feature map)`: 출력 데이터

### 합성곱 연산

- 필터 연산
- 필터 = 커널
- 필터의 윈도우를 일정 간격으로 이동해가며 입력 데이터에 적용한다음 `단일 곱셈-누산(fused multiply-add, FMA)`
- `필터의 매개변수`가 그동안의 `가중치`의 해당함
- `편향`도 존재, 항상 하나만 존재 (1x1)
- 참고) https://cs231n.github.io/convolutional-networks/

### 패딩(Padding)

- 합성곱 연산을 수행하기 전에 입력데이터의 주변을 0으로 채우는 것
- 주로 출력 크기를 조정할 목적으로 사용함
  - 입력데이터의 공간적 크기를 고정한 채로 다음 계층에 전달하기 위해

### 스트라이드(Stride)

- 필터를 적용하는 간격
- 스트라이드를 키우면 출력의 크기가 작아짐 (반대로 패딩을 크게하면 출력 크기가 늘어남)

### 3차원 데이터의 합성곱 연산

- 각 채널마다 각 필터로 합성곱 연산 수행, 그 결과를 더해서 **하나의 출력** 을 얻는다.
- **입력 데이터의 채널 수와 필터의 채널 수가 같아야 함**
- 필터 자체의 크기는 원하는 값으로 설정 가능 (단, 모든 채널의 필터가 같은 크기여야 함)
- 블록으로 생각
  - (C, FH, FW)
  - 출력 데이터는 채널이 1인 특징맵 (1, OH, OW)
  - 그렇다면, 출력을 다수의 채널로 어떻게 내보낼까?
    - **필터(가중치)를 여러 개 사용하자!**
      - (C, H, W) \* (FN, C, FH, FW) = (FN, OH, OW)
- 합성곱 연산에선 필터의 수도 생각해야 함
  - 보통 필터의 가중치 데이터는 4차원 : (출력 채널 수, 입력 채널 수, 높이, 너비)
- 편향은 (FN, 1, 1)
- 출력 결과는 (FN, OH, OW)

### 배치 처리

- 입력 데이터 : 4차원 -> (데이터 수, 채널 수, 높이, 너비)
- 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 수행 (N회 분의 처리를 한번에 수행)

---

## 풀링 계층

- 가로, 세로 방향의 공간을 줄이는 연산
- `최대 풀링(max pooling)`과 `평균 풀링(average pooling)`
  - 이미지 인식 분야에서는 최대 풀링 많이 씀
- 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통
- 풀링 계층의 특징
  1.  학습해야 할 매개변수가 없다.
  2.  채널 수가 변하지 않는다.
  3.  입력의 변화에 영향을 적게 받는다(강건하다).
- 구현
  1.  입력 데이터 전개
  2.  행렬 최대값 구하기
  3.  적절한 모양으로 reshape

---

## 합성곱 계층의 깊이

- 합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보(사물의 의미)가 추출됨
- layer 1은 에지, layer 3은 texture, layer 5는 object parts, output은 object classes 등

## 대표적인 CNN

- `LeNet`: CNN 원조
  - 손글씨 인식
  - 합성곱 계층과 서브샘플링 계층 거듭하면서 마지막으로 완전연결 계층
  - 현재와 다른 것
    - 활성화함수로 **시그모이드** 사용 (현재는 ReLU 사용)
    - **서브샘플링** 을 하여 중간 데이터의 크기를 작아지게 함 (현재는 maxpool 사용)
- `AlexNet`: 딥러닝이 주목받도록 이끔
  - 합성곱 계층과 풀링 계층 거듭하면서 마지막으로 완전연결 계층
  - LeNet에서 바뀐 것
    - 활성화 함수로 ReLU 사용
    - `LRN(Local Response Normalization)`이라는 국소적 정규화를 실시하는 계층 이용
    - 드롭아웃 사용
