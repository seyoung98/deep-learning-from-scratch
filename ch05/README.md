## 오차역전파법(Backpropagation)

- 기울기를 빠르게 구해줌

## 계산 그래프

- 계산 과정을 `그래프 자료구조`로 나타냄
  - 복수의 node와 edge(선)
- `순전파(forward propagation)`: 계산을 왼쪽 -> 오른쪽으로 진행
- `역전파(backward propagation)`: 계산을 왼쪽 <- 오른쪽으로 진행
  - 나중에 미분 계산할 때 중요!

### 국소적 계산

- 국소적 : 자신과 직접 관계된 작은 범위
- 계산그래프는 `국소적 계산`을 전파함으로써 최종 결과를 얻음
- 국소적 계산은 어떤 일이 벌어지든 상관 없이 자신과 관계된 정보만으로 결과를 출력한다.
- 자신과 관련된 계산만 신경 씀

### 계산그래프 장점 (backpropagation을 계산그래프로 설명하는 이유)

1. 국소적 계산
2. 중간 계산 결과를 모두 보관함
3. **미분을 효율적으로 계산 가능**

---

## 연쇄법칙(Chain rule)

- 국소적 미분을 전달하는 원리는 `연쇄법칙`을 이용함
- `합성 함수의 미분`에 대한 성질
  - 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낸다

### 연쇄법칙과 계산그래프

- 노드로 들어온 입력 신호에 그 노드의 국소적 미분(편미분)을 곱한 후 다음노드로 전달
- 맨 왼쪽 역전파를 보면 연쇄법칙의 원리와 같음

---

## 역전파

1. 덧셈 노드의 역전파
   - 입력 신호 그대로 다음 노드로 출력
2. 곱셈 노드의 역전파
   - 순전파의 입력 신호를 서로 바꾼 값을 곱해서 하류로 보냄

---

## 어파인 계층

- 신경망의 순전파 때 수행하는 행렬의 내적을 기하학에서 `어파인 변환(Affine transformation)`이라고 함
- 각 변수가 행렬임에 주의 (행렬의 내적 시 shape, 곱셈 순서 중요)

## Softmax-with-Loss 계층

- Softmax 계층의 역전파는 (y1-t1, y2-t2, ...)과 같이 출력과 정답레이블 차분이다.
- **신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해짐**
  - 이래서 backpropagation으로 Loss 함수 값 줄이는 것이 가능함
  - 이것은 우연이 아니라 Cross Entropy Errorr가 그렇게 설계되었음
  - `항등함수`의 손실함수로 `MSE` 사용해도 마찬가지의 결과 나옴
- 역전파 때 전파하는 값을 batch_size로 나눠서 데이터 1개당 오차를 앞 계층으로 전달함
